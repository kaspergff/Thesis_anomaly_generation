{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# imprt tree\n",
    "from sklearn import tree\n",
    "# import metrics\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SessionID</th>\n",
       "      <th>Activity</th>\n",
       "      <th>anomaly</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>46</td>\n",
       "      <td>/werk_nl/werknemer/mijn_werkmap/doorgeven/taken</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>46</td>\n",
       "      <td>/werk_nl/werknemer/mijn_werkmap/doorgeven/mijn...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>46</td>\n",
       "      <td>/werk_nl/werknemer/mijn_werkmap/postvak/mijn_d...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>46</td>\n",
       "      <td>/werk_nl/werknemer/mijn_werkmap/postvak/mijn_b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>46</td>\n",
       "      <td>/werk_nl/werknemer/mijn_werkmap/postvak/mijn_b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7174929</th>\n",
       "      <td>55314751</td>\n",
       "      <td>/werk_nl/werknemer/werkmap</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7174930</th>\n",
       "      <td>55314751</td>\n",
       "      <td>/werk_nl/werknemer/mijn_werkmap/doorgeven/taken</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7174931</th>\n",
       "      <td>55314751</td>\n",
       "      <td>/werk_nl/werknemer/mijn_werkmap/doorgeven/taken</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7174932</th>\n",
       "      <td>55314751</td>\n",
       "      <td>/werk_nl/werknemer/mijn_werkmap/doorgeven/taken</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7174933</th>\n",
       "      <td>55314751</td>\n",
       "      <td>/werk_nl/werknemer/mijn_werkmap/doorgeven/taken</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7174934 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         SessionID                                           Activity  anomaly\n",
       "0               46    /werk_nl/werknemer/mijn_werkmap/doorgeven/taken        0\n",
       "1               46  /werk_nl/werknemer/mijn_werkmap/doorgeven/mijn...        0\n",
       "2               46  /werk_nl/werknemer/mijn_werkmap/postvak/mijn_d...        0\n",
       "3               46  /werk_nl/werknemer/mijn_werkmap/postvak/mijn_b...        0\n",
       "4               46  /werk_nl/werknemer/mijn_werkmap/postvak/mijn_b...        0\n",
       "...            ...                                                ...      ...\n",
       "7174929   55314751                         /werk_nl/werknemer/werkmap        0\n",
       "7174930   55314751    /werk_nl/werknemer/mijn_werkmap/doorgeven/taken        0\n",
       "7174931   55314751    /werk_nl/werknemer/mijn_werkmap/doorgeven/taken        0\n",
       "7174932   55314751    /werk_nl/werknemer/mijn_werkmap/doorgeven/taken        0\n",
       "7174933   55314751    /werk_nl/werknemer/mijn_werkmap/doorgeven/taken        0\n",
       "\n",
       "[7174934 rows x 3 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw = pd.read_csv(\"./labeled_data/labeled_df_15.csv\", encoding_errors=\"ignore\", on_bad_lines='skip', sep=\",\", index_col=False,\n",
    "                    usecols=['SessionID','Activity','anomaly'])\n",
    "# change anomaly to int\n",
    "df_raw['anomaly'] = df_raw['anomaly'].astype(int)\n",
    "df_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plan:\n",
    "- Het gaat om de transities tussen de verschillende activiteiten\n",
    "  1. Encode alle transities\n",
    "  2. Maak een dictionary met alle transities\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_and_filter(df_raw, file_name, amount_real, amount_anomalies):\n",
    "\n",
    "    df_gen_anomalies = pd.read_csv(file_name)\n",
    "    df_gen_anomalies[\"anomaly\"] = 1\n",
    "    df_raw_filtered = df_raw[df_raw[\"SessionID\"].isin(df_raw[\"SessionID\"].unique()[:amount_real])]\n",
    "    df_gen_anomalies_filtered = df_gen_anomalies[df_gen_anomalies[\"SessionID\"].isin(df_gen_anomalies[\"SessionID\"].unique()[:amount_anomalies])]\n",
    "\n",
    "    df = pd.concat([df_raw_filtered, df_gen_anomalies_filtered])\n",
    "    return df\n",
    "\n",
    "def df_one_hot_encoder(_df):\n",
    "    df = _df.copy()\n",
    "    # create consecutive column\n",
    "      # create consecutive column\n",
    "    df['consecutive'] = df.groupby(\"SessionID\")['Activity'].shift(-1)\n",
    "    # if consecutive is NaN, then consecutive = END\n",
    "    df['consecutive'] = df['consecutive'].fillna(\"END\")\n",
    "        # create column with the transition\n",
    "    df['transition'] = df['Activity'] + \"->\" + df['consecutive']\n",
    "        # check if final_df has column consecutive or activity\n",
    "    if \"consecutive\" in df.columns:\n",
    "        df = df.drop(columns=['consecutive'])\n",
    "    if \"Activity\" in df.columns:\n",
    "        df = df.drop(columns=['Activity'])\n",
    "    \n",
    "    #creating instance of one-hot-encoder\n",
    "    encoder = OneHotEncoder()\n",
    "    \n",
    "    #perform one-hot encoding on 'team' column \n",
    "    encoder_df = pd.DataFrame(encoder.fit_transform(df[['transition']]).toarray(), index=df.index)\n",
    "    encoder_df.columns = encoder.get_feature_names(['transition'])\n",
    "\n",
    "    #merge one-hot encoded columns back with original DataFrame\n",
    "    final_df = df.join(encoder_df)\n",
    "    return final_df\n",
    "\n",
    "\n",
    "def calculating_duration(df):\n",
    "    #Calculating the duration of an activity by using the end time. Duration returned in minutes\n",
    "    df[\"End_time\"] = df.groupby(\"SessionID\")[\"TIMESTAMP\"].shift(periods=-1)\n",
    "    df[\"Duration\"] = (df[\"End_time\"] - df[\"TIMESTAMP\"]) / pd.offsets.Minute(1)\n",
    "    return df\n",
    "\n",
    "def make_df_for_isolation_forest(df):\n",
    "    #Counting each activity in every session\n",
    "    df_sum = df.groupby(\"SessionID\").sum()\n",
    "    df_sum[\"anomaly\"] = df.groupby(\"SessionID\")[\"anomaly\"].max()\n",
    "    return df_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter to get df with only real anomalies\n",
    "amount_for_filtering_real_anomalies = 0\n",
    "df_only_real_anomalies = df_raw[df_raw[\"anomaly\"] == 1]\n",
    "\n",
    "#Make the data ready for training\n",
    "df_only_anomaly_selection = df_only_real_anomalies[[\"SessionID\", \"Activity\", \"anomaly\"]]\n",
    "one_hot_encoded_df_only_anomalies = df_one_hot_encoder(df_only_anomaly_selection)\n",
    "df_only_anomalies_for_training = make_df_for_isolation_forest(one_hot_encoded_df_only_anomalies)\n",
    "\n",
    "df_only_anomalies_for_train_set = df_only_anomalies_for_training[:10]\n",
    "df_only_anomalies_for_test_set = df_only_anomalies_for_training[10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select amount of sessions used in the dataframe\n",
    "var_1_amount_real = 30000\n",
    "var_1_amount_generated = 0\n",
    "df_real = concat_and_filter(df_raw, \"./gen_sessions/1000/100_1000.csv\", amount_real = var_1_amount_real, amount_anomalies = var_1_amount_generated)\n",
    "\n",
    "#Filter only the normal instances. \n",
    "df_real = df_real[df_real[\"anomaly\"] == 0]\n",
    "\n",
    "#df_real = pd.concat([df_real, df_only_real_anomalies_filtered])\n",
    "\n",
    "#Make the data ready for training\n",
    "df_real_selection = df_real[[\"SessionID\", \"Activity\", \"anomaly\"]]\n",
    "one_hot_encoded_df_real = df_one_hot_encoder(df_real_selection)\n",
    "df_real_for_training = make_df_for_isolation_forest(one_hot_encoded_df_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make train test split\n",
    "X = df_real_for_training.drop(columns=[\"anomaly\"])\n",
    "y = df_real_for_training[\"anomaly\"]\n",
    "\n",
    "X_train_temp, X_test_temp, y_train_temp, y_test_temp = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add 10 anomalies back to the training data to make it more realistic.\n",
    "X_train = pd.concat([X_train_temp, df_only_anomalies_for_train_set.drop(columns=[\"anomaly\"])]).fillna(0)\n",
    "y_train = pd.concat([y_train_temp, df_only_anomalies_for_train_set[\"anomaly\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.concat([X_test_temp, df_only_anomalies_for_test_set.drop(columns=[\"anomaly\"])]).fillna(0)\n",
    "y_test = pd.concat([y_test_temp, df_only_anomalies_for_test_set[\"anomaly\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import the generated sessions:\n",
    "ses_amount = 5000\n",
    "base_path = f\"gen_sessions/{str(ses_amount)}/\"\n",
    "\n",
    "gen_sessions_paths = [\n",
    "  base_path + f'5_{ses_amount}.csv',\n",
    "  base_path + f'10_{ses_amount}.csv',\n",
    "  base_path + f'25_{ses_amount}.csv',\n",
    "  base_path + f'50_{ses_amount}.csv',\n",
    "  base_path + f'75_{ses_amount}.csv',\n",
    "  base_path + f'100_{ses_amount}.csv',\n",
    "  # base_path + 'an.csv',\n",
    "  \n",
    "  # base_path + '75_10000.csv',\n",
    "  # base_path + '100_10000.csv',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gen_sessions/5000/5_5000.csv 1\n",
      "0.6173913043478261\n",
      "\n",
      "gen_sessions/5000/5_5000.csv 251\n",
      "0.6173913043478261\n",
      "\n",
      "gen_sessions/5000/5_5000.csv 501\n",
      "0.6173913043478261\n",
      "\n",
      "gen_sessions/5000/5_5000.csv 751\n",
      "0.6173913043478261\n",
      "\n",
      "gen_sessions/5000/5_5000.csv 1001\n",
      "0.6173913043478261\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [69], line 51\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m column_name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m X_train_extra\u001b[39m.\u001b[39mcolumns:\n\u001b[0;32m     49\u001b[0m         X_test\u001b[39m.\u001b[39mdrop(columns\u001b[39m=\u001b[39m[column_name], inplace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m---> 51\u001b[0m clf_extra \u001b[39m=\u001b[39m tree\u001b[39m.\u001b[39;49mDecisionTreeClassifier()\u001b[39m.\u001b[39;49mfit(X_train_extra, y_train_extra_transformed)\n\u001b[0;32m     52\u001b[0m \u001b[39m# clf_extra = LogisticRegression(random_state=0, max_iter=1000).fit(X_train_extra, y_train_extra_transformed)\u001b[39;00m\n\u001b[0;32m     53\u001b[0m y_train_predict \u001b[39m=\u001b[39m clf_extra\u001b[39m.\u001b[39mpredict(X_train_extra)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\sklearn\\tree\\_classes.py:969\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    939\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, X, y, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, check_input\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m    940\u001b[0m     \u001b[39m\"\"\"Build a decision tree classifier from the training set (X, y).\u001b[39;00m\n\u001b[0;32m    941\u001b[0m \n\u001b[0;32m    942\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    966\u001b[0m \u001b[39m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m    967\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 969\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m    970\u001b[0m         X,\n\u001b[0;32m    971\u001b[0m         y,\n\u001b[0;32m    972\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[0;32m    973\u001b[0m         check_input\u001b[39m=\u001b[39;49mcheck_input,\n\u001b[0;32m    974\u001b[0m     )\n\u001b[0;32m    975\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\sklearn\\tree\\_classes.py:172\u001b[0m, in \u001b[0;36mBaseDecisionTree.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    170\u001b[0m check_X_params \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(dtype\u001b[39m=\u001b[39mDTYPE, accept_sparse\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcsc\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    171\u001b[0m check_y_params \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(ensure_2d\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m)\n\u001b[1;32m--> 172\u001b[0m X, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[0;32m    173\u001b[0m     X, y, validate_separately\u001b[39m=\u001b[39;49m(check_X_params, check_y_params)\n\u001b[0;32m    174\u001b[0m )\n\u001b[0;32m    175\u001b[0m \u001b[39mif\u001b[39;00m issparse(X):\n\u001b[0;32m    176\u001b[0m     X\u001b[39m.\u001b[39msort_indices()\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\sklearn\\base.py:591\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    589\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mestimator\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m check_X_params:\n\u001b[0;32m    590\u001b[0m     check_X_params \u001b[39m=\u001b[39m {\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mdefault_check_params, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_X_params}\n\u001b[1;32m--> 591\u001b[0m X \u001b[39m=\u001b[39m check_array(X, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mX\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_X_params)\n\u001b[0;32m    592\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mestimator\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m check_y_params:\n\u001b[0;32m    593\u001b[0m     check_y_params \u001b[39m=\u001b[39m {\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mdefault_check_params, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_y_params}\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:856\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    854\u001b[0m         array \u001b[39m=\u001b[39m array\u001b[39m.\u001b[39mastype(dtype, casting\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39munsafe\u001b[39m\u001b[39m\"\u001b[39m, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    855\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 856\u001b[0m         array \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49masarray(array, order\u001b[39m=\u001b[39;49morder, dtype\u001b[39m=\u001b[39;49mdtype)\n\u001b[0;32m    857\u001b[0m \u001b[39mexcept\u001b[39;00m ComplexWarning \u001b[39mas\u001b[39;00m complex_warning:\n\u001b[0;32m    858\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    859\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mComplex data not supported\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(array)\n\u001b[0;32m    860\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39mcomplex_warning\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\pandas\\core\\generic.py:2064\u001b[0m, in \u001b[0;36mNDFrame.__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m   2063\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__array__\u001b[39m(\u001b[39mself\u001b[39m, dtype: npt\u001b[39m.\u001b[39mDTypeLike \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m np\u001b[39m.\u001b[39mndarray:\n\u001b[1;32m-> 2064\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39;49masarray(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_values, dtype\u001b[39m=\u001b[39;49mdtype)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "auc_score_dicts = {}\n",
    "dict_prec_and_rec_list = {}\n",
    "\n",
    "amount_anomalies_list_total = []\n",
    "precision_score_list_total = []\n",
    "recall_score_list_total = []\n",
    "\n",
    "for url in gen_sessions_paths:\n",
    "    amount_anomalies_list = []\n",
    "    precision_score_list = []\n",
    "    recall_score_list = []\n",
    "\n",
    "    for var_2_amount_generated in range(1, 5000, 250):\n",
    "    #[10, 100, 500, 1000, 2000]:\n",
    "    #for var_2_amount_generated in [2000, 2500, 3000, 4000, 5000, 9999]:\n",
    "\n",
    "        #Select amount of sessions used in the dataframe\n",
    "        var_2_amount_real = 0\n",
    "\n",
    "        df_generated = concat_and_filter(df_raw, url, amount_real = var_2_amount_real, amount_anomalies=var_2_amount_generated)\n",
    "        #Make data ready for training\n",
    "        df_generated_selection = df_generated[[\"SessionID\", \"Activity\", \"anomaly\"]]\n",
    "        one_hot_encoded_df_generated = df_one_hot_encoder(df_generated_selection)\n",
    "        df_generated_for_training = make_df_for_isolation_forest(one_hot_encoded_df_generated)\n",
    "        \n",
    "        #Add the generated anomalies to the training dataset\n",
    "        X_train_extra = pd.concat([X_train, df_generated_for_training.drop(columns=[\"anomaly\"])]).fillna(0)\n",
    "        y_train_extra = pd.concat([y_train, df_generated_for_training[\"anomaly\"]])\n",
    "        # print(y_train_extra)\n",
    "\n",
    "        #transform y values to binary class\n",
    "        Le = LabelEncoder()\n",
    "        Le.fit(y_train_extra)\n",
    "        y_train_transformed = Le.transform(y_train)\n",
    "        y_test_transformed = Le.transform(y_test)\n",
    "        y_train_extra_transformed = Le.transform(y_train_extra)\n",
    "\n",
    "        #Make sure that both dataframes have the same columns\n",
    "        for column_name in X_train_extra.columns:\n",
    "            if column_name not in X_test.columns:\n",
    "                X_test[column_name] = 0\n",
    "\n",
    "        #Make sure that both dataframes have the same columns\n",
    "        for column_name in X_test.columns:\n",
    "            if column_name not in X_train_extra.columns:\n",
    "                X_test.drop(columns=[column_name], inplace=True)\n",
    "        \n",
    "        clf_extra = tree.DecisionTreeClassifier().fit(X_train_extra, y_train_extra_transformed)\n",
    "        # clf_extra = LogisticRegression(random_state=0, max_iter=1000).fit(X_train_extra, y_train_extra_transformed)\n",
    "        # y_train_predict = clf_extra.predict(X_train_extra)\n",
    "        y_test_predict = clf_extra.predict(X_test)\n",
    "        \n",
    "        #AUC predict\n",
    "        fpr, tpr, thresholds = metrics.roc_curve(y_test_transformed, y_test_predict)\n",
    "        print(url, var_2_amount_generated)\n",
    "        print(metrics.auc(fpr, tpr))\n",
    "        print()\n",
    "        auc_score_dicts[(url, var_2_amount_generated)] = metrics.auc(fpr, tpr)\n",
    "        \n",
    "\n",
    "        \n",
    "        precision_score = [i for i in classification_report(y_test_transformed, y_test_predict).split(\" \") if len(i) > 2][8]\n",
    "        recall_score = [i for i in classification_report(y_test_transformed, y_test_predict).split(\" \") if len(i) > 2][9]\n",
    "\n",
    "        amount_anomalies_list.append(var_2_amount_generated)\n",
    "        precision_score_list.append(precision_score)\n",
    "        recall_score_list.append(recall_score)\n",
    "    \n",
    "    \n",
    "    amount_anomalies_list = [int(i) for i in amount_anomalies_list]\n",
    "    precision_score_list = [float(i) for i in precision_score_list]\n",
    "    recall_score_list = [float(i) for i in recall_score_list]\n",
    "    print(amount_anomalies_list, precision_score_list, recall_score_list)\n",
    "    \n",
    "    string_recall = \"recall_\" + str(url)\n",
    "    string_precision = \"precision_\" + str(url)\n",
    "    dict_prec_and_rec_list[string_recall] = recall_score_list\n",
    "    dict_prec_and_rec_list[string_precision] = precision_score_list\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(amount_anomalies_list, precision_score_list, label = \"precision_score\")\n",
    "    plt.plot(amount_anomalies_list, recall_score_list, label = \"recall_score\")\n",
    "    plt.xlabel(\"Number of anomalous sessions injected in training data\")\n",
    "    plt.xlabel(\"Score\")\n",
    "    # save_string = \"prec_rec_scores/\" + str(url.split(\".\")[0].split(\"/\")[1] + url.split(\".\")[0].split(\"/\")[2]) + \".png\"    \n",
    "    plt.legend()\n",
    "    # plt.savefig(save_string)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
