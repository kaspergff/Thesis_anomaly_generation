{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "# base_df = pd.read_csv(\"fake_data.csv\", encoding_errors=\"ignore\", on_bad_lines='skip', sep=\",\",)\n",
    "base_df = pd.read_csv(\"BPI2016_Clicks_Logged_In.csv\", encoding_errors=\"ignore\", on_bad_lines='skip', sep=\";\",\n",
    "                usecols=['CustomerID', 'AgeCategory', 'Gender', 'Office_U', 'Office_W',\n",
    "       'SessionID', 'IPID', 'TIMESTAMP', 'VHOST', 'URL_FILE', 'PAGE_NAME',\n",
    "       'REF_URL_category', 'page_load_error', 'page_action_detail', 'tip',\n",
    "       'service_detail', 'xps_info'])\n",
    "base_df.rename(columns={'URL_FILE':'Activity'}, inplace=True)\n",
    "base_df[\"TIMESTAMP\"] = pd.to_datetime(base_df[\"TIMESTAMP\"], infer_datetime_format=True)\n",
    "base_df = base_df.sort_values([\"SessionID\", \"TIMESTAMP\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def create_df_for_pivot(df: pd.DataFrame):\n",
    "  df = df.copy()\n",
    "  #Create consecutive column with the consecutive activity and count how many time each pair occurs\n",
    "  df[\"Consecutive_1\"] = df.groupby(\"SessionID\")[\"Activity\"].shift(periods=-1)\n",
    "\n",
    "  #Count all the pairs and safe in new DataFrame\n",
    "  df_for_pivot_1 = pd.DataFrame(Counter(list(zip(df['Activity'], df['Consecutive_1'],))), index=[\"value\"]).T.reset_index()\n",
    "\n",
    "  df_for_pivot_1.fillna(\"end_session\", inplace=True)\n",
    "  return df_for_pivot_1\n",
    "  \n",
    "def make_pivot(df, index_names, column_names):\n",
    "  df = df.copy()\n",
    "  \"\"\"\n",
    "  Functions that takes in a DataFrame and returns a pivot table with all the chances\n",
    "  \n",
    "  Create pivot table where chances are calculated that each row is succeded by activity that is represented in the column\n",
    "  \"\"\"\n",
    "  df_chances = df.pivot_table(index=index_names, columns=column_names, values='value')\n",
    "  \n",
    "  #Calculate what the probabilities are by summing the row and dividing all the values in the row by total sum of the row\n",
    "  df_chances[\"total_row_count\"] = df_chances.sum(axis=1)\n",
    "  df_chances = df_chances.div(df_chances[\"total_row_count\"], axis=0)\n",
    "  \n",
    "  df_chances.fillna(0, inplace=True)\n",
    "  df_chances.drop(\"total_row_count\", axis=1, inplace=True)\n",
    "\n",
    "  return df_chances\n",
    "\n",
    "  \n",
    "# Create df for pivot\n",
    "df_for_pivot = create_df_for_pivot(base_df)\n",
    "df_chances_1 = make_pivot(df_for_pivot, \"level_0\", \"level_1\")\n",
    "\n",
    "#Calculate the probability of each activity that it is the first activity performed in the session\n",
    "df_chances_1[\"start_session_chance\"] = base_df.groupby(\"SessionID\").nth(0)[\"Activity\"].value_counts() / base_df.groupby(\"SessionID\").nth(0)[\"Activity\"].value_counts().sum()\n",
    "df_chances_1.fillna(0, inplace=True)\n",
    "\n",
    "df_chances_total = df_chances_1 \n",
    "df_chances_total\n",
    "\n",
    "#Split the start probabilities from the normal DataFrame\n",
    "start_chances = df_chances_total[\"start_session_chance\"][df_chances_total[\"start_session_chance\"] > 0]\n",
    "\n",
    "#Create df with all chances except the starting chance.\n",
    "final_df = df_chances_total.iloc[:, :-1]\n",
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skip_event(_df:pd.DataFrame,event:str,deviation_rate):\n",
    "  df = _df.copy()\n",
    "  # print(f\"Decrease the probability to reach event: {event} with deviation rate: {deviation_rate}\")\n",
    "  old_probability = df.loc[:,event]\n",
    "  deviation_rate = deviation_rate / 100\n",
    "  \n",
    "  # decrease the probability\n",
    "  for i,prob in old_probability.items():\n",
    "    # skip row of event\n",
    "    if i == event: continue\n",
    "    \n",
    "    # start changing the prob \n",
    "    if prob > 0:\n",
    "      decrease = prob * (1-deviation_rate)\n",
    "      df.loc[i,event] -= prob - decrease      \n",
    "      for _i,_prob in df.loc[i,:].items():\n",
    "        df.loc[i,_i] += (prob - decrease) * df.loc[event,_i]\n",
    "  \n",
    "  \n",
    "  # change row of event\n",
    "  if df.loc[event,event] > 0:\n",
    "    decrease = df.loc[event,event] * (1 - deviation_rate)\n",
    "    original_value = df.loc[event,event]\n",
    "    count = df.loc[event]\n",
    "    count = count[count > 0].__len__() - 1\n",
    "    \n",
    "    for i,prob in  df.loc[event,:].items():\n",
    "      if df.loc[event,i] > 0:\n",
    "        # Circle case\n",
    "        if i == event: \n",
    "          df.loc[event,i] -= prob - decrease\n",
    "        else: df.loc[event,i] += (original_value - decrease) / count \n",
    "      \n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skip_multiple_events(list,prop_matrix):\n",
    "  res = []\n",
    "  for item in list:\n",
    "    new_df = skip_event(prop_matrix,item[0],item[1])\n",
    "    res.append({\"new_df\":new_df,\n",
    "                \"activity\": item[0],\n",
    "                \"dev\":item[1]})\n",
    "  return res\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# events_to_skip = [('Accept_offer',10)]\n",
    "events_to_skip = [('/werk_nl/werknemer/mijn_werkmap/doorgeven/taken',50),\n",
    "                  ('/werk_nl/werknemer/mijn_werkmap/werk-zoeken/vacatures_bij_mijn_cv',50),\n",
    "                  ('/werk_nl/werknemer/mijn_werkmap/werk-zoeken/mijn_cv',50)]\n",
    "skipped = skip_multiple_events(events_to_skip,final_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_markov_chain(amount:int,start_chances,df_dict:pd.DataFrame) -> list:\n",
    "  generated_sessions = []\n",
    "  while len(generated_sessions) < amount:\n",
    "    new_session = []\n",
    "    # choose start activity\n",
    "    start_activity = np.random.choice(a = np.array(start_chances.index), size = 1, p = np.array(start_chances.values))\n",
    "    # start_activity = np.random.choice(a = np.array(df_dict.iloc[0,:].index), size = 1, p = np.array(df_dict.iloc[0,:].values))\n",
    "    new_session.append(start_activity[0])\n",
    "\n",
    "    \n",
    "    while new_session[-1] != 'end_session':\n",
    "    # while new_session[-1] != 'E':\n",
    "      curr_activity = new_session[-1]\n",
    "      row_activity = df_dict.loc[curr_activity,:]\n",
    "      activity = np.random.choice(a = list(row_activity.keys()), size = 1, p = list(row_activity.values))\n",
    "      new_session.append(activity[0])\n",
    "    \n",
    "    generated_sessions.append(new_session)        \n",
    "    \n",
    "  \n",
    "  return generated_sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sessions(skipping_list:list,total_amount:int):\n",
    "  session_per_activity = total_amount / len(skipping_list)\n",
    "  sessions = []\n",
    "  for item in skipping_list:\n",
    "    markov = generate_markov_chain(session_per_activity,start_chances,item[\"new_df\"])\n",
    "    sessions += markov\n",
    "  return sessions\n",
    "  \n",
    "\n",
    "markov_result = generate_sessions(skipped,2500)\n",
    "# res\n",
    "print(len(markov_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def markov_result_to_df(list_markov_result, name_index, name_csv):\n",
    "    activity_list_final = []\n",
    "    session_id_list = []\n",
    "    for nbr in range(len(markov_result)):\n",
    "        string_generated = name_index + str(nbr)\n",
    "        for i in markov_result[nbr]:\n",
    "            if i != \"end_session\":\n",
    "                activity_list_final.append(i)\n",
    "                session_id_list.append(string_generated)\n",
    "                \n",
    "    df_generated = pd.DataFrame(list(zip(session_id_list, activity_list_final)),\n",
    "               columns =['SessionID', 'URL_FILE'])\n",
    "    df_generated.to_csv(name_csv)\n",
    "    \n",
    "markov_result_to_df(markov_result, \"generated_top3_50%_\", \"gen_sessions/skipped/generated_withCircle_top3_50%_2500.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
