{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\krdeg\\AppData\\Local\\Temp\\ipykernel_11900\\236448581.py:1: DtypeWarning: Columns (11,13,14,15,16) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  base_df = pd.read_csv(\"BPI2016_Clicks_Logged_In.csv\", encoding_errors=\"ignore\", on_bad_lines='skip', sep=\";\",\n"
     ]
    }
   ],
   "source": [
    "base_df = pd.read_csv(\"BPI2016_Clicks_Logged_In.csv\", encoding_errors=\"ignore\", on_bad_lines='skip', sep=\";\",\n",
    "                        usecols=[ 'CustomerID', 'AgeCategory', 'Gender', 'Office_U', 'Office_W',\n",
    "                                  'SessionID', 'IPID', 'TIMESTAMP', 'VHOST', 'URL_FILE', 'PAGE_NAME',\n",
    "                                  'REF_URL_category', 'page_load_error', 'page_action_detail', 'tip',\n",
    "                                  'service_detail', 'xps_info'])\n",
    "base_df.rename(columns={'URL_FILE':'Activity'}, inplace=True)\n",
    "base_df[\"TIMESTAMP\"] = pd.to_datetime(base_df[\"TIMESTAMP\"], infer_datetime_format=True)\n",
    "base_df = base_df.sort_values([\"SessionID\", \"TIMESTAMP\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def create_df_for_pivot(df: pd.DataFrame):\n",
    "  df = df.copy()\n",
    "  #Create consecutive column with the consecutive activity and count how many time each pair occurs\n",
    "  df[\"Consecutive_1\"] = df.groupby(\"SessionID\")[\"Activity\"].shift(periods=-1)\n",
    "\n",
    "  #Count all the pairs and safe in new DataFrame\n",
    "  df_for_pivot_1 = pd.DataFrame(Counter(list(zip(df['Activity'], df['Consecutive_1'],))), index=[\"value\"]).T.reset_index()\n",
    "\n",
    "  df_for_pivot_1.fillna(\"end_session\", inplace=True)\n",
    "  return df_for_pivot_1\n",
    "  \n",
    "def make_pivot(df, index_names, column_names):\n",
    "  df = df.copy()\n",
    "  \"\"\"\n",
    "  Functions that takes in a DataFrame and returns a pivot table with all the chances\n",
    "  \n",
    "  Create pivot table where chances are calculated that each row is succeded by activity that is represented in the column\n",
    "  \"\"\"\n",
    "  df_chances = df.pivot_table(index=index_names, columns=column_names, values='value')\n",
    "  \n",
    "  #Calculate what the probabilities are by summing the row and dividing all the values in the row by total sum of the row\n",
    "  df_chances[\"total_row_count\"] = df_chances.sum(axis=1)\n",
    "  df_chances = df_chances.div(df_chances[\"total_row_count\"], axis=0)\n",
    "  \n",
    "  df_chances.fillna(0, inplace=True)\n",
    "  df_chances.drop(\"total_row_count\", axis=1, inplace=True)\n",
    "\n",
    "  return df_chances\n",
    "\n",
    "  \n",
    "# Create df for pivot\n",
    "df_for_pivot = create_df_for_pivot(base_df)\n",
    "df_chances_1 = make_pivot(df_for_pivot, \"level_0\", \"level_1\")\n",
    "\n",
    "#Calculate the probability of each activity that it is the first activity performed in the session\n",
    "df_chances_1[\"start_session_chance\"] = base_df.groupby(\"SessionID\").nth(0)[\"Activity\"].value_counts() / base_df.groupby(\"SessionID\").nth(0)[\"Activity\"].value_counts().sum()\n",
    "df_chances_1.fillna(0, inplace=True)\n",
    "\n",
    "df_chances_total = df_chances_1 \n",
    "df_chances_total\n",
    "\n",
    "#Split the start probabilities from the normal DataFrame\n",
    "start_chances = df_chances_total[\"start_session_chance\"][df_chances_total[\"start_session_chance\"] > 0]\n",
    "\n",
    "#Create df with all chances except the starting chance.\n",
    "final_df = df_chances_total.iloc[:, :-1]\n",
    "final_df\n",
    "probability_matrix = final_df.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A skipping pattern means that there is a high probability to go from a to b to c and a smaller probability to go from a to c directly where a,b and c are not the same\n",
    "def find_skipping_patterns(df:pd.DataFrame):\n",
    "  df = df.copy()\n",
    "  res = []\n",
    "  for _a,valueRowA in df.iterrows():\n",
    "    # filter the row to not include the row itself\n",
    "    rowA = valueRowA.filter(regex='^(?!'+_a+'$).*$')\n",
    "    # find the highest value in the row and get the index\n",
    "    for _b, value_A_to_B in rowA.items():\n",
    "      if value_A_to_B > 0:\n",
    "      # remove the index from the row to not make sure we do not generate a skipping pattern with the same activity twice\n",
    "        rowWithoutIndex = rowA.filter(regex='^(?!'+_b+'$).*$')\n",
    "        # remove zeros from the row\n",
    "        finalRowB = rowWithoutIndex[rowWithoutIndex > 0]\n",
    "        # check there is a probability to go from the row of the highest index to a item in rowNotHighest\n",
    "        # if there is append to res\n",
    "        if _b != 'end_session':\n",
    "          rowB = df.loc[_b,:]\n",
    "          for _c, value_B_to_C in rowB.items():\n",
    "            if value_B_to_C > 0 and _c in finalRowB.index.values:\n",
    "              res.append({\"a\":_a,\"b\":_b,\"c\":_c, \"a-b\":value_A_to_B, \"b-c\":value_B_to_C,'a-c':df.loc[_a,_c],'a-b-c':value_A_to_B*value_B_to_C})\n",
    "      \n",
    "  return res\n",
    "\n",
    "skipping_patterns = find_skipping_patterns(probability_matrix)\n",
    "skipping_patterns.sort(key=lambda x: x[\"a-b-c\"] - x['a-c'], reverse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "403396\n",
      "105\n"
     ]
    }
   ],
   "source": [
    "factor_1 = 0.08\n",
    "factor_1_string = (str(factor_1).replace('.', ''))\n",
    "factor_2 = 5\n",
    "factor_2_string = (str(factor_2).replace('.', ''))\n",
    "\n",
    "print(len(skipping_patterns))\n",
    "# filter the skipping patters to only include the ones where the probability of a-b is min factor_1 and the a-b * b-c is factor_2 times higher than a-c\n",
    "final_skipping_patterns = [x for x in skipping_patterns if x[\"a-b\"] > factor_1 and x[\"a-b\"] * x[\"b-c\"] > x['a-c'] * factor_2] \n",
    "print(len(final_skipping_patterns))\n",
    "# final_skipping_patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "skipping_df = pd.DataFrame(final_skipping_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find the skipping patterns in a session.\n",
    "def find_anomalous_sessions(sessions_df:pd.DataFrame,skipping_patterns:pd.DataFrame) -> list[int]:\n",
    "\n",
    "  # Create a new column with the consecutive activity\n",
    "  sessions_df[\"Consecutive_Activity\"] = sessions_df.groupby(\"SessionID\")[\"Activity\"].shift(periods=-1)  \n",
    "\n",
    "  merged = pd.merge(sessions_df, skipping_patterns, left_on=['Activity','Consecutive_Activity'], right_on=['a','c'], how='inner',indicator='Anomaly')\n",
    "  merged['Anomaly'] = np.where(merged.Anomaly == 'both', True, False)\n",
    "  \n",
    "  anomalous_session = merged[merged['Anomaly'] == True]\n",
    "  # create a list with all SessionIDs that have an anomaly\n",
    "  anomaly_sessions = anomalous_session['SessionID'].unique()\n",
    "  \n",
    "  return anomaly_sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "348"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = find_anomalous_sessions(base_df,skipping_df)\n",
    "len(test)\n",
    "\n",
    "# 5478504\n",
    "# 1434495"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new column in the base_df to indicate if the session is anomalous\n",
    "# if a session is anomalous it will have a True in the column else False\n",
    "labeled_df = base_df.copy()\n",
    "labeled_df[\"anomalous\"] = base_df[\"SessionID\"].apply(lambda x: x in ses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_df.to_csv(f\"labeled_data/skipped/{factor_1_string}_{factor_2_string}.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
